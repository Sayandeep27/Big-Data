# Apache Spark: Transformations, Actions, and Lazy Evaluation

---

## What is Apache Spark?

**Apache Spark** is a fast, distributed data processing engine used for handling large-scale data. It processes data in parallel across multiple machines and is widely used for **Big Data**, **ETL**, **Machine Learning**, and **Real-time processing**.

Spark mainly works using two important concepts:

* **Transformations**
* **Actions**

Along with a powerful optimization concept called **Lazy Evaluation**.

---

# 1. Transformations in Spark

## What are Transformations?

**Transformations** are operations that create a **new dataset** from an existing dataset.

They do **NOT** execute immediately.

Instead, Spark only remembers what needs to be done.

This is why they are called **Lazy Operations**.

---

## Simple Definition

**Transformation = Operation that modifies data but does NOT produce final output immediately**

---

## Real Life Example

Imagine you have a list of students and you want only students with marks > 50.

You are just giving instructions — not actually printing results.

---

## Common Types of Transformations

| Transformation | Description                                   |
| -------------- | --------------------------------------------- |
| map()          | Applies a function to each element            |
| filter()       | Filters data based on condition               |
| flatMap()      | Similar to map but can return multiple values |
| select()       | Select specific columns (DataFrame)           |
| groupBy()      | Groups data                                   |
| distinct()     | Removes duplicates                            |

---

## Example (PySpark)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Example").getOrCreate()

data = [1,2,3,4,5]
rdd = spark.sparkContext.parallelize(data)

# Transformation
squared_rdd = rdd.map(lambda x: x*x)
```

**Important:**

At this point **nothing is executed yet**.

Spark is only creating a plan.

---

# 2. Actions in Spark

## What are Actions?

**Actions** are operations that **trigger execution** and return the final result.

They actually make Spark run the transformations.

---

## Simple Definition

**Action = Operation that produces output or result**

---

## Real Life Example

You told someone:

* Filter students with marks > 50 (Transformation)
* Now print the list (Action)

Printing is the final step — so execution happens.

---

## Common Types of Actions

| Action    | Description                |
| --------- | -------------------------- |
| collect() | Returns all data to driver |
| count()   | Counts number of records   |
| first()   | Returns first element      |
| take(n)   | Returns first n elements   |
| show()    | Displays DataFrame         |
| save()    | Writes data to storage     |

---

## Example (PySpark)

```python
# Action
result = squared_rdd.collect()

print(result)
```

Now Spark **actually runs** the computation and returns:

```
[1, 4, 9, 16, 25]
```

---

# 3. Lazy Evaluation in Spark

## What is Lazy Evaluation?

**Lazy Evaluation** means Spark does **NOT execute transformations immediately**.

Instead, it waits until an **Action** is called.

---

## Why Spark Uses Lazy Evaluation?

It helps in:

* Optimization
* Reducing computation
* Combining multiple steps
* Faster execution

Spark creates a **DAG (Directed Acyclic Graph)** of operations and optimizes it before running.

---

## Simple Example

```python
rdd = spark.sparkContext.parallelize([1,2,3,4])

rdd1 = rdd.map(lambda x: x*2)
rdd2 = rdd1.filter(lambda x: x > 4)

# No execution yet

output = rdd2.collect()
```

Execution happens **only at collect()**.

---

## Step-by-Step What Happens Internally

1. Spark records `map` operation
2. Spark records `filter` operation
3. No computation happens
4. When `collect()` is called
5. Spark optimizes the plan
6. Then executes the job

---

# Transformations vs Actions

| Feature   | Transformations   | Actions            |
| --------- | ----------------- | ------------------ |
| Execution | Lazy              | Immediate          |
| Output    | New RDD/DataFrame | Final Result       |
| Purpose   | Modify data       | Get result         |
| Example   | map(), filter()   | collect(), count() |

---

# How They Work Together

1. User writes multiple **Transformations**
2. Spark builds execution plan (DAG)
3. When **Action** is called
4. Spark executes entire pipeline

---

# Key Interview Points (VVI)

* Transformations are **lazy**
* Actions **trigger execution**
* Spark uses **DAG optimization**
* Lazy evaluation improves performance
* Multiple transformations are executed together

---

# One-Line Memory Trick

**Transformations = Instructions**
**Actions = Execution**
**Lazy Evaluation = Wait until result is required**

---

# Final Summary

* Transformations create new datasets but do not run immediately.
* Actions execute the computation and produce results.
* Lazy evaluation allows Spark to optimize and run jobs efficiently.
* Together, these concepts make Spark fast and scalable for Big Data processing.

---
