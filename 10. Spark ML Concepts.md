# Apache Spark ML Concepts (Easy Guide)

---

## What is Spark ML?

**Spark ML** (Machine Learning Library) is a part of **Apache Spark** that helps us build machine learning models on large datasets in a distributed way.

It provides ready‑made tools for:

* Data preprocessing
* Feature engineering
* Model building
* Model evaluation
* Full ML workflow automation

---

# Important Spark ML Concepts

---

## 1. UDF (User Defined Function)

### What is UDF?

**UDF** allows you to create your own custom function when Spark does not provide a built‑in function for your requirement.

In simple words:

> UDF = Your own function that works on Spark DataFrame columns.

---

### Why do we need UDF?

Because sometimes built‑in Spark functions are not enough.

Example:

* Converting text to custom format
* Applying business logic
* Creating new calculated column

---

### Example (PySpark)

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

# Custom function
def square(x):
    return x * x

square_udf = udf(square, IntegerType())

df.withColumn("square_value", square_udf(df["number"]))
```

---

## 2. StringIndexer

### What is StringIndexer?

**StringIndexer** converts categorical (string) values into numeric indexes.

Machine learning models understand numbers, not text.

So we convert:

| Category | Converted Value |
| -------- | --------------- |
| Male     | 0               |
| Female   | 1               |

---

### Example

```python
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCol="gender", outputCol="gender_index")
model = indexer.fit(df)
indexed_df = model.transform(df)
```

---

## 3. VectorAssembler

### What is VectorAssembler?

**VectorAssembler** combines multiple columns into a single vector column called **features**.

Machine learning models in Spark expect input in vector format.

---

### Example

```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(
    inputCols=["age", "salary", "experience"],
    outputCol="features"
)

output = assembler.transform(df)
```

---

## 4. Estimator

### What is Estimator?

An **Estimator** is an algorithm that **learns from data**.

It takes dataset and produces a trained model.

In simple words:

> Estimator = Training algorithm

---

### Examples of Estimators

* Linear Regression
* Logistic Regression
* Decision Tree
* StringIndexer (also an estimator)

---

### Flow

```
Estimator + Data  ->  Model
```

---

## 5. Transformer

### What is Transformer?

A **Transformer** is something that **transforms input data into output data**.

It uses the trained model.

In simple words:

> Transformer = Applies learned model on data

---

### Examples

* Trained Logistic Regression Model
* VectorAssembler
* IndexToString

---

### Flow

```
Model + Data  ->  Transformed Data (Predictions)
```

---

## 6. Pipeline

### What is Pipeline?

**Pipeline** is a sequence of steps used to automate the full ML workflow.

Instead of doing steps one by one, we connect them together.

---

### Real Life Analogy

Student process:

* Study
* Practice
* Give exam

All steps together = Pipeline

---

### Example Steps in ML Pipeline

1. StringIndexer
2. VectorAssembler
3. Model Training

---

### Example Code

```python
from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[indexer, assembler, lr])
model = pipeline.fit(df)
result = model.transform(df)
```

---

## 7. Evaluator

### What is Evaluator?

**Evaluator** is used to measure how good our machine learning model is.

It gives accuracy, RMSE, precision, etc.

---

### Examples

* BinaryClassificationEvaluator
* MulticlassClassificationEvaluator
* RegressionEvaluator

---

### Example

```python
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="accuracy"
)

accuracy = evaluator.evaluate(predictions)
```

---

## 8. MLlib

### What is MLlib?

**MLlib** is the machine learning library of Apache Spark.

It provides tools for:

* Classification
* Regression
* Clustering
* Recommendation
* Feature Engineering

---

### Two APIs in Spark MLlib

| API                        | Description              |
| -------------------------- | ------------------------ |
| RDD Based (Old)            | Older API, less used now |
| DataFrame Based (Spark ML) | Modern and recommended   |

---

## Complete End‑to‑End Example

```python
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

# Step 1: Convert categorical to numeric
indexer = StringIndexer(inputCol="gender", outputCol="gender_index")

# Step 2: Combine features
assembler = VectorAssembler(
    inputCols=["age", "salary", "gender_index"],
    outputCol="features"
)

# Step 3: Model
lr = LogisticRegression(featuresCol="features", labelCol="label")

# Step 4: Pipeline
pipeline = Pipeline(stages=[indexer, assembler, lr])

model = pipeline.fit(df)
predictions = model.transform(df)
```

---

# Quick Revision Table

| Concept         | One Line Meaning                            |
| --------------- | ------------------------------------------- |
| UDF             | Custom function created by user             |
| StringIndexer   | Converts text categories to numbers         |
| VectorAssembler | Combines columns into single feature vector |
| Estimator       | Learns from data (training algorithm)       |
| Transformer     | Applies learned model on data               |
| Pipeline        | Automates full ML workflow                  |
| Evaluator       | Measures model performance                  |
| MLlib           | Spark's machine learning library            |

---

# Final Summary

Spark ML provides a structured way to build machine learning systems at scale.

The typical workflow is:

1. Clean data
2. Convert categorical data (StringIndexer)
3. Create feature vector (VectorAssembler)
4. Train model (Estimator)
5. Generate predictions (Transformer)
6. Evaluate model (Evaluator)
7. Automate using Pipeline

This complete ecosystem is provided by **MLlib** inside Apache Spark.

---
