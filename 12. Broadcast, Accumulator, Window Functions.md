# Apache Spark: Broadcast Variable, Accumulator, and Window Functions

---

## Table of Contents

1. Introduction
2. What is a Broadcast Variable?
3. Why Do We Need Broadcast Variables?
4. Broadcast Variable – Working Concept
5. Broadcast Variable – Example (PySpark)
6. What is an Accumulator?
7. Why Do We Need Accumulators?
8. Accumulator – Working Concept
9. Accumulator – Example (PySpark)
10. What is a Window Function in Spark?
11. Why Do We Need Window Functions?
12. Types of Window Functions
13. Window Specification (Window Spec)
14. Window Function – Examples (PySpark)
15. Broadcast vs Accumulator vs Window Function
16. When to Use What (Interview Point of View)
17. Key Takeaways

---

## 1. Introduction

Apache Spark provides several advanced features to process large-scale data efficiently in distributed systems.

Among the most important and frequently asked concepts in interviews are:

* **Broadcast Variable**
* **Accumulator**
* **Window Functions**

This README explains all three concepts in **very simple and practical language** with clear examples.

---

# Broadcast Variable

---

## 2. What is a Broadcast Variable?

A **Broadcast Variable** is a read-only variable that is **shared with all worker nodes** in a Spark cluster.

Instead of sending the same data again and again to every task, Spark sends it **once** and every worker uses that copy.

**In simple words:**

> Send data once → All workers use it → Faster execution

---

## 3. Why Do We Need Broadcast Variables?

In distributed processing:

* Each worker node normally receives its own copy of data
* This increases **network cost**
* Slows down execution

Broadcast variables help:

* Reduce data transfer
* Reduce memory usage
* Improve performance

---

## 4. Broadcast Variable – Working Concept

Without Broadcast:

* Spark sends small dataset multiple times to each executor

With Broadcast:

* Spark sends dataset once
* Executors cache it locally

---

## 5. Broadcast Variable – Example (PySpark)

### Example Scenario

We have:

* Large dataset → Transactions
* Small dataset → Country codes

We broadcast the small dataset.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BroadcastExample").getOrCreate()

country_codes = {"IN": "India", "US": "United States"}

broadcast_var = spark.sparkContext.broadcast(country_codes)

rdd = spark.sparkContext.parallelize(["IN", "US", "IN"])

result = rdd.map(lambda x: broadcast_var.value[x])

print(result.collect())
```

---

# Accumulator

---

## 6. What is an Accumulator?

An **Accumulator** is a variable used to **collect information from worker nodes back to the driver**.

Workers can **add values**, but cannot read it.

Driver can read the final result.

**In simple words:**

> Workers update → Driver reads

---

## 7. Why Do We Need Accumulators?

Used for:

* Counting errors
* Counting records
* Monitoring processing
* Debugging

---

## 8. Accumulator – Working Concept

* Each worker updates the accumulator
* Spark merges results
* Final value available at driver

---

## 9. Accumulator – Example (PySpark)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("AccumulatorExample").getOrCreate()

acc = spark.sparkContext.accumulator(0)

rdd = spark.sparkContext.parallelize([1,2,3,4,5])

def add_if_even(x):
    global acc
    if x % 2 == 0:
        acc += 1

rdd.foreach(add_if_even)

print("Even numbers count:", acc.value)
```

---

# Window Function

---

## 10. What is a Window Function in Spark?

A **Window Function** performs calculations across a **set of rows related to the current row** without collapsing rows.

Unlike groupBy:

* groupBy reduces rows
* window function keeps all rows

---

## 11. Why Do We Need Window Functions?

Used for:

* Ranking
* Running totals
* Moving average
* Comparing rows

---

## 12. Types of Window Functions

### 1. Ranking Functions

* row_number()
* rank()
* dense_rank()

### 2. Aggregate Functions

* sum()
* avg()
* min()
* max()

### 3. Analytical Functions

* lag()
* lead()

---

## 13. Window Specification (Window Spec)

Defines:

* Partition column
* Order column

```python
from pyspark.sql.window import Window

windowSpec = Window.partitionBy("department").orderBy("salary")
```

---

## 14. Window Function – Examples (PySpark)

### Sample Data

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import row_number
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("WindowExample").getOrCreate()

data = [("HR", 3000), ("HR", 4000), ("IT", 5000), ("IT", 6000)]

df = spark.createDataFrame(data, ["department", "salary"])
```

### Row Number Example

```python
windowSpec = Window.partitionBy("department").orderBy("salary")

df.withColumn("row_number", row_number().over(windowSpec)).show()
```

---

# Comparison Section

---

## 15. Broadcast vs Accumulator vs Window Function

| Feature    | Broadcast Variable | Accumulator           | Window Function        |
| ---------- | ------------------ | --------------------- | ---------------------- |
| Purpose    | Share small data   | Collect metrics       | Row-wise analytics     |
| Direction  | Driver → Workers   | Workers → Driver      | Within DataFrame       |
| Read/Write | Read-only          | Add-only from workers | Read & compute         |
| Use Case   | Join optimization  | Counters              | Ranking, running total |

---

## 16. When to Use What (Interview Point of View)

**Use Broadcast Variable when:**

* Small lookup table
* Optimizing joins
* Reducing network cost

**Use Accumulator when:**

* Counting events
* Logging metrics
* Monitoring pipeline

**Use Window Function when:**

* Ranking employees
* Running totals
* Department-wise analytics

---

## 17. Key Takeaways

* **Broadcast Variable** improves performance by sending small data once.
* **Accumulator** is used for collecting counters from workers.
* **Window Functions** allow advanced analytics without losing rows.
* These three are **very important for Spark optimization and interviews**.

---

**End of Document**
