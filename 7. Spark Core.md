# Apache Spark Core — Complete Beginner Friendly Guide

---

## What is Apache Spark?

**Apache Spark** is a **fast, open‑source distributed computing framework** used to process **large amounts of data** quickly.

It is designed to work on **big data** and can run on:

* Single machine
* Cluster of machines
* Cloud

Spark is much faster than older systems like **Hadoop MapReduce** because it processes data **in memory**.

---

# What is Spark Core?

**Spark Core** is the **foundation (heart)** of Apache Spark.

All other Spark components are built on top of Spark Core:

* Spark SQL
* Spark Streaming
* MLlib
* GraphX

Spark Core provides:

* Distributed data processing
* Task scheduling
* Memory management
* Fault tolerance
* Basic APIs for working with data

In simple words:

> **Spark Core is the engine that actually runs your big data jobs.**

---

# Why Do We Need Spark Core?

Imagine you have **1 TB of data**.

If you process on a single computer:

* Very slow
* Memory not enough

Spark Core solves this by:

* Splitting data into parts
* Sending parts to multiple machines
* Processing in parallel

Result:

* Very fast processing
* Scalable system

---

# Key Features of Spark Core

## 1. Distributed Processing

Data is divided into smaller chunks and processed on multiple machines.

Example:

* 100 GB data
* 10 machines
* Each machine processes 10 GB

---

## 2. In‑Memory Processing

Spark stores intermediate data in RAM instead of disk.

Result:

* Much faster than Hadoop MapReduce

---

## 3. Fault Tolerance

If one machine fails, Spark can recompute data using lineage.

---

## 4. Lazy Evaluation

Spark does not execute immediately.

It waits until an **action** is called.

---

## 5. Parallel Processing

Many tasks run at the same time across cluster nodes.

---

# Core Concepts of Spark Core

---

## 1. RDD (Resilient Distributed Dataset)

RDD is the **main data structure** in Spark Core.

Definition:

> RDD is a **distributed, fault‑tolerant collection of data**.

Characteristics:

* Distributed
* Immutable
* Fault tolerant
* Can be processed in parallel

Example:

```
Data split across machines:

Machine 1 -> [1,2,3]
Machine 2 -> [4,5,6]
Machine 3 -> [7,8,9]
```

---

## How to Create RDD

### From Collection

```python
from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")

numbers = sc.parallelize([1,2,3,4,5])
```

---

### From File

```python
rdd = sc.textFile("data.txt")
```

---

# Transformations and Actions

RDD operations are of two types.

---

## 1. Transformations (Lazy)

They create a **new RDD**.

Examples:

* map()
* filter()
* flatMap()

Example:

```python
numbers = sc.parallelize([1,2,3,4])

squared = numbers.map(lambda x: x*x)
```

No output yet because it is lazy.

---

## 2. Actions (Execute Job)

They produce the result.

Examples:

* collect()
* count()
* first()

Example:

```python
squared.collect()
```

Now Spark executes the job.

---

# Lazy Evaluation Explained Simply

Spark builds a **DAG (Directed Acyclic Graph)** of operations.

Execution happens only when an action is called.

Example Flow:

```
RDD -> map -> filter -> action
```

Spark optimizes the execution before running.

---

# DAG and Execution Flow

Steps:

1. User writes transformations
2. Spark creates DAG
3. DAG Scheduler divides into stages
4. Tasks sent to executors
5. Results returned

---

# Spark Architecture (Very Important)

Main Components:

## 1. Driver Program

* Runs main code
* Creates SparkContext
* Sends tasks

## 2. Cluster Manager

* Manages resources
* Examples: Standalone, YARN, Mesos, Kubernetes

## 3. Executors

* Run tasks
* Store data

Simple Flow:

```
Driver -> Cluster Manager -> Executors
```

---

# Spark Core Example — Word Count

Most famous example.

Input file:

```
hello spark
hello world
```

Code:

```python
rdd = sc.textFile("file.txt")

words = rdd.flatMap(lambda line: line.split(" "))

pairs = words.map(lambda word: (word, 1))

counts = pairs.reduceByKey(lambda a, b: a + b)

counts.collect()
```

Output:

```
[("hello", 2), ("spark", 1), ("world", 1)]
```

---

# RDD Persistence (Caching)

We can store RDD in memory for faster reuse.

```python
rdd.cache()
```

Why?

* Avoid recomputation
* Faster iterative algorithms

---

# Fault Tolerance Using Lineage

Spark remembers how RDD was created.

If partition lost:

* Spark recomputes using previous transformations

This history is called **lineage**.

---

# Spark Core vs Hadoop MapReduce

| Feature         | Spark     | Hadoop MapReduce |
| --------------- | --------- | ---------------- |
| Speed           | Very Fast | Slow             |
| Processing      | In Memory | Disk Based       |
| Iterative Tasks | Good      | Poor             |
| Ease of Use     | Easy APIs | Complex          |

---

# When Should We Use Spark Core?

Use Spark Core when:

* Processing very large data
* Need fast computation
* Need distributed processing
* Building data pipelines
* Log processing

---

# Real Life Use Cases

* Recommendation systems
* Fraud detection
* Log analysis
* ETL pipelines
* Machine learning preprocessing

---

# Summary

* Spark Core is the **foundation** of Apache Spark
* It provides distributed, fault‑tolerant processing
* RDD is the main data structure
* Operations are Transformations and Actions
* Uses DAG for optimized execution
* Much faster than Hadoop MapReduce

---

# One Line Interview Answer

**Spark Core is the fundamental engine of Apache Spark that provides distributed data processing using RDDs with fault tolerance, lazy evaluation, and in‑memory computation.**

---
