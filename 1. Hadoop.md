# Hadoop – Complete Beginner to Intermediate Guide

---

## What is Hadoop?

**Hadoop** is an **open-source big data framework** used to **store and process very large amounts of data** across multiple computers (called a cluster).

It is designed to handle:

* Huge data (TBs, PBs)
* Structured and unstructured data
* Distributed storage
* Parallel processing

**In simple words:**

> Hadoop allows us to store big data on many machines and process it fast by working on all machines together.

---

## Why Do We Use Hadoop?

### Problem Before Hadoop

When data becomes very large:

* One computer cannot store all data
* Processing becomes very slow
* System may crash
* Very expensive to upgrade a single machine

### Solution by Hadoop

Hadoop solves this by:

* Splitting data into small blocks
* Storing across many machines
* Processing in parallel
* Using low-cost commodity hardware

---

## Real Life Example

### Example: YouTube / Instagram

Every day:

* Millions of videos
* Billions of likes, comments
* User behavior data

This data is:

* Very large
* Continuous
* Unstructured

Hadoop helps:

* Store all user activity
* Analyze trends
* Recommend videos
* Detect spam

---

## How Hadoop Works?

Hadoop works using **Distributed Storage + Distributed Processing**.

### Step-by-Step Flow

1. Data is divided into small blocks (typically 128MB).
2. Blocks are stored across multiple machines.
3. Data is replicated for safety.
4. Processing is sent to the machines where data exists.
5. All machines process in parallel.
6. Final results are combined.

**Key Idea:**

> Move computation to data, not data to computation.

---

# Hadoop Architecture

Hadoop architecture mainly consists of four core components.

---

## Hadoop Ecosystem (Toolbox)

Hadoop is not just one tool. It is a **complete ecosystem (toolbox)**.

### Common Hadoop Ecosystem Tools

| Tool      | Purpose                       |
| --------- | ----------------------------- |
| HDFS      | Storage layer                 |
| YARN      | Resource management           |
| MapReduce | Processing engine             |
| Hive      | SQL on Hadoop                 |
| Pig       | Data processing using scripts |
| HBase     | NoSQL database                |
| Sqoop     | Import/export data from RDBMS |
| Flume     | Data ingestion                |

---

## 1. HDFS (Hadoop Distributed File System)

### What is HDFS?

HDFS is the **storage layer** of Hadoop.

It stores large files by splitting them into blocks and distributing across cluster nodes.

---

### HDFS Components

| Component | Work                        |
| --------- | --------------------------- |
| NameNode  | Master – manages metadata   |
| DataNode  | Worker – stores actual data |

---

### How HDFS Stores Data

* File is divided into blocks (128MB default)
* Each block is stored in different machines
* Each block is replicated (usually 3 copies)

---

### Example

If a 1GB file is stored:

* Split into ~8 blocks
* Blocks stored on different DataNodes
* Replicated for fault tolerance

---

## 2. YARN (Yet Another Resource Negotiator)

### What is YARN?

YARN is the **resource management layer** of Hadoop.

It manages:

* CPU
* Memory
* Job scheduling
* Cluster resources

---

### YARN Components

| Component         | Work                            |
| ----------------- | ------------------------------- |
| ResourceManager   | Master – allocates resources    |
| NodeManager       | Worker – manages node resources |
| ApplicationMaster | Manages individual application  |

---

## 3. MapReduce

### What is MapReduce?

MapReduce is the **data processing engine** of Hadoop.

It processes large data using parallel processing.

---

### Two Phases

| Phase  | Work                         |
| ------ | ---------------------------- |
| Map    | Breaks task into small tasks |
| Reduce | Combines results             |

---

### Example: Word Count

Input:

```
"Big data is big"
```

Map Output:

```
Big → 1
Data → 1
is → 1
Big → 1
```

Reduce Output:

```
Big → 2
Data → 1
is → 1
```

---

# How All Components Work Together

### End-to-End Flow

1. Data stored in HDFS.
2. User submits job.
3. YARN allocates resources.
4. MapReduce processes data near storage.
5. Result stored back in HDFS.

---

# Advantages of Hadoop

| Advantage           | Explanation                            |
| ------------------- | -------------------------------------- |
| Scalable            | Add more machines easily               |
| Fault Tolerant      | Data replication prevents loss         |
| Cost Effective      | Uses commodity hardware                |
| Handles Big Data    | TB, PB scale                           |
| Parallel Processing | Faster computation                     |
| Flexible            | Handles structured & unstructured data |

---

# Disadvantages of Hadoop

| Disadvantage          | Explanation                           |
| --------------------- | ------------------------------------- |
| High Latency          | Not suitable for real-time processing |
| Complex Setup         | Cluster management needed             |
| Requires Skilled Team | Admin + Big Data knowledge            |
| Security Challenges   | Needs extra configuration             |
| Small Files Problem   | Inefficient with many small files     |

---

# When Should We Use Hadoop?

Use Hadoop when:

* Data is very large
* Batch processing is required
* Cost should be low
* Fault tolerance is important

Do NOT use Hadoop when:

* Real-time processing required
* Data is small
* Low latency is critical

---

# Quick Interview Revision Section

**Hadoop = Distributed Storage + Distributed Processing**

**Core Components:**

* HDFS → Storage
* YARN → Resource Manager
* MapReduce → Processing
* Ecosystem → Tools like Hive, Pig, HBase

**Key Concept:**

> Hadoop processes data where it is stored.

---

# One Line Definition

**Hadoop is an open-source framework used to store and process massive data using distributed storage (HDFS) and parallel processing (MapReduce) managed by YARN.**

---

# End of Document
