# What is Parquet?

---

## Introduction

**Parquet** is a **columnar file format** used to store large amounts of data efficiently, especially in **Big Data** tools like **Apache Spark, Hadoop, Hive, and Presto**.

In simple words,

> **Parquet stores data column-wise instead of row-wise**, which makes it faster and smaller in size for analytics.

---

## Why Do We Need Parquet?

When data becomes very large (millions or billions of rows), normal file formats like **CSV** become:

* Slow
* Large in size
* Inefficient for analytics

Parquet solves these problems by:

* Reducing storage space
* Increasing query speed
* Reading only required columns

---

## Row Format vs Column Format

### Row-Based Format (CSV Example)

| ID | Name | Age | Salary |
| -- | ---- | --- | ------ |
| 1  | Amit | 25  | 30000  |
| 2  | Riya | 28  | 40000  |
| 3  | John | 30  | 50000  |

In CSV, data is stored like this:

```
1,Amit,25,30000
2,Riya,28,40000
3,John,30,50000
```

All values of a row are stored together.

---

### Column-Based Format (Parquet)

Parquet stores data like this internally:

**ID Column**

```
1, 2, 3
```

**Name Column**

```
Amit, Riya, John
```

**Age Column**

```
25, 28, 30
```

**Salary Column**

```
30000, 40000, 50000
```

Each column is stored separately.

---

## Why Column Storage is Powerful?

Imagine you only want the **Salary** column.

### Without Parquet (CSV)

* Entire file must be read
* All columns loaded
* Slow

### With Parquet

* Only Salary column is read
* Faster query
* Less memory usage

---

## Real Life Example

Suppose a company has:

* 100 million employee records
* 50 columns

Manager wants only:

* Employee ID
* Salary

With CSV:

* Entire 50 columns read

With Parquet:

* Only 2 columns read
* Huge performance improvement

---

## Key Features of Parquet

| Feature                | Description                     |
| ---------------------- | ------------------------------- |
| Columnar Storage       | Stores data column-wise         |
| Compression            | File size becomes very small    |
| Fast Queries           | Reads only required columns     |
| Schema Support         | Data types are stored with file |
| Efficient for Big Data | Used in Spark, Hive, Hadoop     |

---

## Compression in Parquet

Parquet automatically compresses data using algorithms like:

* Snappy
* Gzip
* Brotli

Because similar values are stored together in columns, compression works very well.

Result:

* Smaller storage
* Faster disk read

---

## Parquet vs CSV

| Feature          | CSV        | Parquet      |
| ---------------- | ---------- | ------------ |
| Storage Style    | Row-based  | Column-based |
| File Size        | Large      | Small        |
| Query Speed      | Slow       | Fast         |
| Schema           | Not stored | Stored       |
| Compression      | Limited    | High         |
| Big Data Support | Poor       | Excellent    |

---

## Where is Parquet Used?

Parquet is widely used in:

* Data Engineering
* Data Warehouses
* Data Lakes
* Machine Learning pipelines
* Analytics dashboards

Tools that commonly use Parquet:

* Apache Spark
* Hadoop
* Hive
* Databricks
* AWS Athena
* Snowflake (external stages)

---

## Simple Example Using PySpark

### Writing Data as Parquet

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ParquetExample").getOrCreate()

data = [(1, "Amit", 25, 30000),
        (2, "Riya", 28, 40000)]

columns = ["ID", "Name", "Age", "Salary"]

df = spark.createDataFrame(data, columns)

df.write.parquet("employee.parquet")
```

---

### Reading Parquet File

```python
df = spark.read.parquet("employee.parquet")

df.show()
```

---

## When Should We Use Parquet?

Use Parquet when:

* Data is very large
* Analytics queries are frequent
* Only some columns are needed
* Working with Spark / Hadoop ecosystem

Avoid Parquet when:

* Data is very small
* Need human-readable format
* Frequent row-wise updates required

---

## Advantages of Parquet

* Very high compression
* Faster analytics queries
* Saves storage cost
* Schema is stored
* Ideal for Data Lakes

---

## Disadvantages of Parquet

* Not human readable
* Slightly slower for row-wise operations
* Not good for frequent updates

---

## One-Line Interview Answer

> **Parquet is a columnar storage file format used in Big Data that stores data column-wise to provide high compression and fast query performance.**

---

## Final Summary

Parquet is one of the most important file formats in modern Data Engineering.

It is specially designed for:

* Big data processing
* Analytics workloads
* Efficient storage

If your dataset is large and queries require only a few columns, **Parquet is the best choice**.

---
