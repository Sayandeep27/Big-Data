# HDFS in Hadoop (Easy to Understand)

---

## What is HDFS?

**HDFS (Hadoop Distributed File System)** is the storage system of Hadoop used to store very large amounts of data across multiple machines (computers) in a cluster.

Instead of storing data in a single computer, HDFS stores data across many computers so that:

* Storage capacity increases
* Processing becomes faster
* System becomes fault tolerant (data is safe even if one machine fails)

In simple words:

> **HDFS = A system that stores big data across many machines safely and efficiently.**

---

## Why Do We Need HDFS?

Imagine you have **10 TB of data**.

A single computer:

* May not have enough storage
* May become slow
* If it crashes → data lost

HDFS solves this by:

* Dividing data into smaller pieces
* Storing those pieces on different machines
* Keeping multiple copies of data

---

## Real Life Example

### Example: YouTube

YouTube stores:

* Videos
* Comments
* Likes
* User data

This data is **petabytes** in size.

They cannot store everything in one server.

So they use distributed storage like HDFS:

* Video file is split into parts
* Parts stored on multiple machines
* If one machine fails → data still available

---

## Basic Idea of HDFS Working

HDFS follows **Master–Slave Architecture**.

| Component | Role                            |
| --------- | ------------------------------- |
| NameNode  | Master node (manages metadata)  |
| DataNode  | Slave nodes (store actual data) |

---

## HDFS Architecture

### 1. NameNode (Master)

Responsibilities:

* Keeps record of where data is stored
* Maintains metadata
* Controls the file system
* Does NOT store actual data

Stores information like:

* File name
* Block location
* Permissions

---

### 2. DataNode (Worker)

Responsibilities:

* Stores actual data blocks
* Sends heartbeat to NameNode
* Replicates data

---

## How HDFS Stores Data

When a file is stored in HDFS:

1. File is divided into blocks
2. Each block stored on different DataNodes
3. Multiple copies are created (Replication)

Default:

* Block size: **128 MB**
* Replication factor: **3**

---

## Example: Storing a 300 MB File

| File Size | Block Size | Blocks Created |
| --------- | ---------- | -------------- |
| 300 MB    | 128 MB     | 3 Blocks       |

Block Distribution:

* Block 1 → DataNode 1, 2, 3
* Block 2 → DataNode 2, 3, 4
* Block 3 → DataNode 3, 4, 5

If DataNode 2 fails → Data still available from other copies.

---

## Important Features of HDFS

### 1. Fault Tolerance

* Multiple copies of data stored
* No data loss if machine fails

### 2. Scalability

* Easily add more machines
* Storage increases automatically

### 3. High Throughput

* Designed for big data processing
* Faster data access

### 4. Distributed Storage

* Data stored across cluster

---

## Read and Write Process in HDFS

### Write Process

1. Client sends file to NameNode
2. NameNode decides where to store blocks
3. Data sent to DataNodes
4. Replication created

### Read Process

1. Client asks NameNode for block locations
2. NameNode provides DataNode addresses
3. Client reads data from nearest DataNode

---

## HDFS vs Traditional File System

| Feature            | Traditional File System | HDFS                  |
| ------------------ | ----------------------- | --------------------- |
| Storage            | Single machine          | Multiple machines     |
| Fault tolerance    | Low                     | High                  |
| Data size          | Small–Medium            | Very Large (Big Data) |
| Scalability        | Limited                 | Very High             |
| Speed for big data | Slow                    | Fast                  |

---

## When Should We Use HDFS?

Use HDFS when:

* Data size is huge (GB, TB, PB)
* Need distributed processing
* Need fault tolerance
* Working with Big Data tools (Hadoop, Spark)

---

## When Should We NOT Use HDFS?

Avoid HDFS when:

* Data is small
* Need real-time processing
* Need low latency
* Too many small files

---

## Key Terms You Must Remember

* HDFS
* NameNode
* DataNode
* Block
* Replication
* Fault Tolerance
* Distributed Storage

---

## One-Line Interview Answer

**HDFS is a distributed file system of Hadoop that stores large data across multiple machines with replication for fault tolerance and high scalability.**

---

## Final Summary

HDFS is designed to handle **Big Data** by splitting large files into blocks, storing them across multiple machines, and keeping multiple copies for safety. This makes the system reliable, scalable, and efficient for large-scale data processing.

---
