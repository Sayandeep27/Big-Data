# SQLContext and SparkSession in Apache Spark

---

## Table of Contents

1. Introduction
2. What is SQLContext?
3. Why SQLContext was Needed
4. Creating SQLContext (Example)
5. What is SparkSession?
6. Why SparkSession was Introduced
7. Creating SparkSession (Example)
8. SQLContext vs SparkSession (Key Differences)
9. Practical Example Using SparkSession
10. When Should You Use SQLContext?
11. Best Practice (Modern Spark)
12. Interview Questions
13. Summary

---

## 1. Introduction

In Apache Spark, we often work with structured data like tables, CSV files, JSON, or databases.

To work with structured data and run SQL queries, Spark provides special entry points:

* **SQLContext** (Old approach)
* **SparkSession** (Modern approach)

Both are used to work with **Spark SQL**.

---

## 2. What is SQLContext?

**SQLContext** is an entry point in older versions of Spark used to:

* Work with structured data
* Create DataFrames
* Run SQL queries
* Load data from files

In simple words:

> **SQLContext is the interface that allows Spark to understand SQL-like operations on data.**

---

## 3. Why SQLContext was Needed

Before Spark SQL was introduced, Spark mainly worked with RDDs.

Problems:

* Hard to write complex queries
* No table-like structure
* Difficult for SQL users

Solution:

**SQLContext** allowed:

* Table format (DataFrame)
* SQL queries
* Easier data analysis

---

## 4. Creating SQLContext (Example)

```python
from pyspark import SparkContext
from pyspark.sql import SQLContext

sc = SparkContext()
sqlContext = SQLContext(sc)

# Load CSV
 df = sqlContext.read.csv("employees.csv", header=True, inferSchema=True)

# Create temp table
 df.createOrReplaceTempView("emp")

# Run SQL query
 result = sqlContext.sql("SELECT * FROM emp WHERE salary > 50000")
 result.show()
```

---

## 5. What is SparkSession?

**SparkSession** is the new and unified entry point introduced in **Spark 2.0**.

It combines multiple contexts into one:

* SQLContext
* HiveContext
* StreamingContext (partially)

In simple words:

> **SparkSession is the single entry point to work with all Spark features.**

---

## 6. Why SparkSession was Introduced

Earlier, Spark had many contexts:

* SparkContext
* SQLContext
* HiveContext

This created confusion.

So Spark introduced **SparkSession** to:

* Simplify usage
* Reduce code complexity
* Provide one unified interface

---

## 7. Creating SparkSession (Example)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("My First Spark App") \
    .getOrCreate()

# Load CSV
 df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# Create temp view
 df.createOrReplaceTempView("emp")

# Run SQL query
 result = spark.sql("SELECT * FROM emp WHERE salary > 50000")
 result.show()
```

---

## 8. SQLContext vs SparkSession (Key Differences)

| Feature     | SQLContext         | SparkSession        |
| ----------- | ------------------ | ------------------- |
| Introduced  | Older Spark        | Spark 2.0           |
| Entry Point | Only for Spark SQL | Unified entry point |
| Supports    | DataFrames, SQL    | All Spark features  |
| Complexity  | More               | Less                |
| Recommended | No                 | Yes                 |
| Replacement | Deprecated concept | Modern standard     |

---

## 9. Practical Example Using SparkSession

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Sales Analysis").getOrCreate()

sales_df = spark.read.csv("sales.csv", header=True, inferSchema=True)

sales_df.createOrReplaceTempView("sales")

high_sales = spark.sql("""
SELECT region, SUM(amount) as total
FROM sales
GROUP BY region
ORDER BY total DESC
""")

high_sales.show()
```

What happens here:

* SparkSession is created
* Data is loaded as DataFrame
* Temporary table is created
* SQL query is executed

---

## 10. When Should You Use SQLContext?

Only when:

* Working with very old Spark projects
* Maintaining legacy code

Otherwise:

**Always use SparkSession.**

---

## 11. Best Practice (Modern Spark)

* Use **SparkSession** for all new projects
* Avoid creating SQLContext manually
* SparkSession already contains SQLContext internally

You can access it like:

```python
spark.sql("SELECT 1")
```

---

## 12. Interview Questions

**Q1. What is SQLContext?**

SQLContext is an entry point used to work with Spark SQL, create DataFrames, and run SQL queries in older Spark versions.

**Q2. What is SparkSession?**

SparkSession is the unified entry point introduced in Spark 2.0 to work with all Spark functionalities.

**Q3. Which one is used today?**

SparkSession.

**Q4. Does SparkSession replace SQLContext?**

Yes. SparkSession internally includes SQLContext.

**Q5. Can we still use SQLContext?**

Yes, but only for legacy systems.

---

## 13. Summary

* SQLContext was used earlier for Spark SQL.
* SparkSession is the modern unified interface.
* SparkSession simplifies Spark development.
* Always prefer SparkSession in real-world projects.

---
